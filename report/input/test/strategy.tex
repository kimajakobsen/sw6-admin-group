\section{Strategy}
\label{sec:strategy}
\newcommand{\idealCC}{\todo{make this correct}XXX}
Our development is to some extent test driven.
As mentioned in \secref{sub:testing} we are using SimpleTest to test our PHP code.
When we know what a function is supposed to do prior to its implementation we write test cases for the function.
If we do not write test cases prior to its implementation we write test cases for the function after its implementation.
Even though it is likely that the person who made the function is the same person as the one who writes the test cases, there are still bugs to be found by this method.
Since working with Moodle is new to us we do not have a full understanding of the Moodle core API, which means that in some cases we will expect a function to return something other than what it actually returns.
Many bugs related to this are discovered by writing test cases.

The test cases we write do not catch all bugs in the system, but that does not mean that they do not serve a purpose.
Apart from the bugs they actually catch, they also improve the robustness of the system.
If someone decides that a function should be altered the test cases for the function ensure that it still behaves as it should.
This is especially important in our case since a new group of people will continue our work later.

A metric for determining how well some software is tested is code coverage. 
The specific type of coverage we measure our code with is \idealCC{} coverage. 
Determining the percentage of coverage we strive for is a cost/benefit situation.
On one hand we want a stable system that is working as intended, but on the other hand we do not want to spend all our time writing test cases.
The importance of testing is based on the criticality of the system.
The radar chart in \secref{subsec:choosingmethod} shows that if our system fails it will only affect the comfort of the users.
Low criticality calls for a relatively low code coverage.
An evaluation of our testing can be seen in \secref{sec:results}.

To cover more code with fewer test cases we choose to perform equivalence partition~\cite{testbogen_or_slidesfromsecondlecture} and boundary value analysis~\cite{testbogen_or_slidesfromsecondlecture}.
Without going into too great details of these concepts, equivalence partitioning means that for every function we test, we partition input values into partitions in which the out come of each element is expected to be the same.
Within these partitions we have to test at least one element.
The outcome of the test should be the same for any other element of the given partition.
A boundary value analysis is performed by identifying boundary values and selecting three values around each boundry value for testing, e.g. if 0 is a boundary value for some integer input, -1, 0, and 1 is chosen for test values.
Boundary values are identified as values of interest\todo{se bogen n√•r den er med}.
These can be the value where elements change from being in one partition to being in another.
Alternatively it can be special objects of the certain input type, such as null, empty string, or empty array.