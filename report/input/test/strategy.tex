\section{Strategy}
\label{sec:strategy}
\newcommand{\idealCC}{\todo{gerne en kilde p\aa{} at det er et godt tal}$60\%$}
Our development is to some extent test driven.
This means that we are creating unit tests for our functionalities and integration tests to test whether or not our \subsystem{} as a whole work as expected and work together with \moodle{}.
As mentioned in \secref{sub:testing} we are using SimpleTest to test our PHP code.
When we know what a function is supposed to do prior to its implementation we write test cases for the function.
If we do not write test cases prior to its implementation we write test cases for the function after its implementation. 
%Even though it is likely that the person who made the function is the same person as the one who writes the test cases, there are still bugs to be found by this method.
Since working with Moodle is new to us we do not have a full understanding of the Moodle core API.
% which means that in some cases we will expect a function to return something other than what it actually returns.
This means that even with the documentation of the \moodle{} core API we may misinterpret the behavior, input, or output of a function.
Many bugs related to this are discovered by writing test cases.

There is code in this project which is not included in functions and are not included in any unit tests.
This include code belonging to our user interfaces.
%, referred to as views.
These UIs are tested using UI testing. 
UI testing is conducted by the developer to ensure that the functionality is as expected, and through the demo meetings where the system is tried out. 

The test cases we write do not catch all bugs in the system, but that does not mean that they do not serve a purpose.
Apart from the bugs they actually catch, they also improve the robustness of the system.
If someone decides that the implementation of a function should be altered the test cases for the function ensure that it still behaves as it should.
This is especially important in our case since a new group of people will continue our work later.

A metric for determining how well some software is tested is code coverage. 
The specific type of coverage we measure our code with is line coverage. 
Determining the percentage of coverage we strive for is a cost/benefit situation.
On one hand we want a stable system that is working as intended, but on the other hand we do not want to spend all our time writing test cases.
The importance of testing is based on the criticality of the system.
The radar chart in \secref{subsec:choosingmethod} shows that if our system fails it will only affect the comfort of the users.
Low criticality calls for a relatively low code coverage.
We choose that \idealCC{} line coverage is sufficient for our core functionality.
Our core functionality being \admlib{} and project group library.
An evaluation of our testing can be seen in \secref{sec:results}.

In addition to considering code coverage we choose to perform equivalence partition~\cite[pp.~67-69]{Patton05} and boundary value analysis (or data testing)~\cite[pp.~70-79]{Patton05} in the creation of test cases.
Without going into too great details of these concepts, equivalence partitioning means that for every function we test, we partition input values into partitions.
Within these partitions the outcome of each element is expected to be the same.
Consequently we have to test at least one element in each partition, but necessarily every element.
A boundary value analysis is performed by identifying boundary values and selecting values around each boundary value for testing, e.g. if 0 is a boundary value for some integer input, -1, 0, and 1 is chosen for test values.
Boundary values are identified as values that delimit a boundary, described by \citeauthor{Patton05} to be ``like the edge of a cliff''~\cite[p.~71]{Patton05}.
These can be the value where elements change from being in one partition to being in another.
Alternatively it can be special objects of the certain input type, such as null, empty string, or empty array.